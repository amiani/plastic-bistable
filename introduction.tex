\section{Introduction}

The past ten years have seen an explosion in interest in machine learning and artificial intelligence. At the center of this interest are a class of algorithms known as neural networks. Thanks to their dramatic success on a variety of difficult problems, neural networks are now widely used in many different areas of science, engineering and business. Recent work applying neural networks has improved the state of the art in a number of important areas, including computer vision, robot control, game playing, speech recognition and natural language processing. All of these areas boast many examples of temporal prediction problems, where the goal is to predict the future given the past. For example, a computer vision system might be tasked with predicting what object will be visible in the next few seconds, or your phone keyboard might be tasked with predicting what word you will type next. These problems are generally well-studied, and a class of neural networks known as recurrent neural networks (RNNs) have proven to work extremely well in many cases.

A typical neural network model is a fixed, parameterized function of its inputs. Its parameters are changed during training to best fit the training data it is given, but the network cannot adapt to its inputs in any way once training is complete.
On the other hand, a recurrent neural network is a dynamic model. It has a memory of its past that is updated with each input. This is a powerful feature that allows the network to learn to leverage temporal correlations in its inputs. This key feature is what makes recurrent neural networks so effective at temporal prediction and sequential decision making problems.

While standard RNNs have been shown to excel at problems that require memory, they suffer from an issue in their training dynamics.
Neural networks are typically trained using the backpropagation algorithm, which first calculates a scalar loss function (e.g. mean squared error) of some training data in the form of input-output examples, then calculates the gradient of the loss with respect to the parameters, and updates the parameters in the direction of the gradient. This is applicable to RNNs, but leads to a problem whereby the parameter gradient either shrinks to 0 or grows to infinity as the sequence length grows. This is known as the vanishing/exploding gradient problem and will be discussed in more detail in the next section. This problem limits the sequence length that standard RNNs can be trained on, and thus their ability to learn dependencies that are distant in time.
Various methods have been proposed to address this issue, including the now-widespread Long-Short Term Memory\cite{hochreiter1997lstm} (LSTM), and most relevantly to this report, the Gated Recurrent Unit\cite{cho2014gru} (GRU). These introduce methods of alleviating the vanishing/exploding gradient, leading to models that are much more capable of learning long-range dependencies. They are so effective in fact, that standard RNNs have been entirely replaced in practice. The GRU is of particular interest here, as it is forms the basis for another type of recurrent neural network reviewed in this report, the Bistable Recurrent Cell\cite{vecoven2021brc}.

The Bistable Recurrent Cell is a recently introduced model that seeks to improve on the GRU by appealing to cellular dynamics that are more closely aligned with those found in biological neural networks. Vecoven et al.\cite{vecoven2021brc} showed that the BRC was more effective than the LSTM and GRU at tasks that neccesitate holding onto a particular input for a long period of time, while only requiring two simple tweaks to the equations that govern the updates to the GRU's memory.
This report builds on the results of Vecoven et al.\cite{vecoven2021brc} and introduces a new model, the Plastic Bistable Recurrent Cell. This model is a further variation of the GRU formula, adding plastic connections between elements of the networks's memory. These plastic connections follow the formulation of Miconi et at.\cite{miconi2018diffplas}, roughly implementing Hebb's rule, a theory of how learning happens in the brain. Miconi et al. showed how plastic networks were able to memorize multiple high-dimensional inputs in a short amount of time, and this, combined with the long-term memorization capability of bistable recurrent cells, is the basis for the model presented in this report.