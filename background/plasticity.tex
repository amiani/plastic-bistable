\subsection*{Differentiable Plasticity}

One of the ways that biological neural networks update the connection strengths between neurons is through a process called synaptic plasticity. Plasticity is the ability of a synapse (connection between two neurons) to change its strength based on the activations of the neurons it connects. The most widespread theory of plasticity is called Hebb's rule \cite{hebb1949rule}, which was proposed as an explanation for learning and memory in the brain. Hebb's rule states that neurons that fire together, wire together i.e. that the strength of a synapse is increased when the activation of its presynaptic neuron is (perhaps along with other neurons) the cause of the activation of its postsynaptic neuron. Although many learning rules based on this fundamnetal idea are possible, Miconi et al. \cite{miconi2018diffplas} proposed a plastic learning rule that is differentiable, a desirable property as it allows for the efficient training of neural networks with many parameters. In their formulation, synapses have two weights, one that is static during each episode (i.e. lifetime of the model), and one that is plastic, being updated based on its pre- and post-synaptic neuron activations. The plastic weight at each timestep according to the recursive formula

\subsection*{Plastic Recurrent Cells}