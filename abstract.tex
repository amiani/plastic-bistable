\begin{center}
	\section*{Abstract}

	In recent years, recurrent neural networks (RNNs) have shown impressive success on a wide variety of sequential prediction tasks. These successes are possible due to variations on the basic RNN formula that alleviate limits on their ability to hold onto memories for long periods of time. Two of these methods are differentiable plasticity and bistable recurrent cells. RNNs with plastic connections are able to memorize high-dimensional inputs. Bistable recurrent cells are able to hold to memories for very long periods of time. This report introduces and analyzes a new recurrent model that combines the properties of both.
\end{center}